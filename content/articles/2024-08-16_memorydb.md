<p><a target="_blank" href="https://app.eraser.io/workspace/KoSBBMGFSpKInsUpJSNv" id="edit-in-eraser-github-link"><img alt="Edit in Eraser" src="https://firebasestorage.googleapis.com/v0/b/second-petal-295822.appspot.com/o/images%2Fgithub%2FOpen%20in%20Eraser.svg?alt=media&amp;token=968381c8-a7e7-472a-8ed6-4a6626da5501"></a></p>

+++
title = "Redis + Strong consistency = AWS MemoryDB"

[taxonomies]
categories = ["cloud"]
+++

[﻿Redis](https://redis.io/) (or the fully open source [﻿Valkey](https://valkey.io/)) is a very versatile and fast in-memory data store. It started as a very simple key-value database, but it has picked a huge number of commands and features over the years. There's plenty of different datatypes, including streams and JSON. There's embedded functions written in Lua, which are fully transactionally isolated, so it's possible to do complex transactional operations with the minimum of data transfer. It's no longer just a cache, it's a full blown database that is suitable for a wide variety of uses.

![Redis flow](/.eraser/KoSBBMGFSpKInsUpJSNv___SdSlyWapPJMYH3JhtxQ9thJXxgb2___---figure---XohIQKCYeAE7dts5Ze_jk---figure---X7LcqShiielZgcIUWzpdOw.png "Redis flow")

The biggest drawback for using Redis for anything else than a simple in-memory cache is that it doesn't provide strong consistency. Under normal operation, when a master is stable, Redis is wonderfully consistent as everything happens just in-memory. But everything being in-memory means that Redis will acknowledge successful writes before they have been persisted anywhere, which means that if the master goes down, you might lose just written data. To combat this, there is the `WAIT` command which can be used to wait until a certain number of replicas have acknowledged the write, which means that at least it's not up to just a single machine anymore. But even then there are no actual guarantees that a replica being promoted would have all the data. It's still just best-effort consistency. In the worst case, a replica could be promoted that doesn't have _any_ data, so the whole database would start from scratch.

![Redis + wait flow](/.eraser/KoSBBMGFSpKInsUpJSNv___SdSlyWapPJMYH3JhtxQ9thJXxgb2___---figure---jFDGvBL6OVx4dKMpb-xbK---figure---je1SqMqk416SMjWe31mt-Q.png "Redis + wait flow")

But what if I told you there's an alternative (if you are on AWS)...

## AWS MemoryDB
MemoryDB is a Redis OSS compatible database that claims to offer both durability and strong consistency, while keeping the fast in-memory performance of Redis. That sounds almost too good to be true. So how do they do it? Is it really compatible with Redis? What are the guarantees they provide?

### Consistency
What MemoryDB promises is that the moment a write operation completes, that write operation is durably persisted (eleven nines, just like S3) and strongly consistent, meaning that no operation will be run on the database without that change present. This means it acts like any ACID database with SERIALIZABLE isolation level - that's everything one could hope for.

### Speed
But it can't do that while keeping the exact same speed that Redis has, can it? Well, no, obviously it can't, since Redis answers immediately after something has been written to memory only on a single machine. But it can get surprisingly close.

Read operations are not affected at all and exhibit the same performance as with Redis. Well, to be exact, it's even a bit better than Redis, but that's due to Amazon optimizing the database for their hardware with some enhanced IO multiplexing. So the response times for both databases are in microsecond - this is an in-memory database after all.

Write operations in Redis will complete usually in 1-2 milliseconds. For MemoryDB that same figure is 3-5 milliseconds. That's obviously slower, and a meaningful difference, but still quite a bit faster than what most databases can manage. Especially if there's any network latency, a couple more milliseconds will end up fully hidden in that latency. This figure is also pretty much exactly the optimal result one can achieve.

In order for the data to be durably persisted, it needs to be stored in multiple data centers. In AWS this means multiple availability zones inside a region. These availability zones are physically located at least a few kilometers away from each other and a maximum of 100 kilometers. The networking round-trip latency is less than 2 milliseconds. Hence if Redis takes 1 millisecond to process the operation, and getting the same operation delivered to two other datacenters takes a 2 millisecond round-trip, we arrive exactly to the 3 millisecond minimum time.

### Implementation
So how do they do it? They add a strongly consistent transactional log into the normal Redis codebase, where the implementation of the log is AWS secret sauce. The transactional log is placed after the normal in-memory operation for Redis, so once the operation is done, the results of the operation are committed into the log and will end up getting relayed to the replicas. And each replica will always ensure it has the entire log replayed before it can become a master.

But if this were done naively, then every write operation would end up taking three times as long because of the transactional log, which would reduce the throughput of the entire database similarily. But that doesn't seem to be the case as total throughput, while less than a standard Redis installation, is quite close in the end. So that can't be the way they do it.

What they actually do is that they only delay the sending of the acknowledgement message after a command until they have received an acknowledgement from the transactional log. This means that the database itself can actually perform operations at full speed in the in-memory database, and only the clients are delayed until there is confirmation that the data has been durably persisted and will be strongly consistent. This is quite a cool idea, and it's easy to reason about the performance knowing this.

![MemoryDB flow](/.eraser/KoSBBMGFSpKInsUpJSNv___SdSlyWapPJMYH3JhtxQ9thJXxgb2___---figure---ol_z2NE2UU7K2ZZlEOfBp---figure---VZOQTnPsKt4omsHZXbLsJQ.png "MemoryDB flow")

### Compatibility
Everything else about Redis stays the same, as everything is first executed in-memory and then written to the transaction log. So we know it supports all the same stuff as Redis does without having to reimplement everything and keeping all the quirks that we've already gotten used to (unliked some... krhm... DocumentDB). And they've added the same JSON support that's currently part of the not-so-open Redis offering.

Replication in Redis is also a very standard feature and thoroughly battle tested. This solution just improves the behaviour around the replication stream, but the actual data elements are the very same. So we don't have to worry about new quirks around randomness or functions or other similar things appearing, but those will still be just like they are in Redis.

### Cost
AWS offers the standard Redis as well, under the name Amazon ElastiCache with Redis OSS compatibility. This makes it easy to compare the prices of that against MemoryDB. Comparing the costs for on-demand instances, they seem to be charging around 50% more for the same instance types. Given that Redis costs are usually quite low compared to the value they bring, that isn't too bad.

In addition to that, there's a cost of $0.20 / GB for data written. This is obviously to cover the cost of the transactional log. Usually Redis is read heavy, and the written data is quite small. One can hope that the extra cost from the data written here won't be terribly significant for most usages.

### Summary
Overall, this gives a very interesting new choice for development. Usually anything that requires strong consistency has had to have been implemented with DynamoDB. And that has a cost per transaction that may get significant. Anything read heavy, or more ephemeral would then be handled by Redis, where the cost would simply be based on the dataset size (the memory size of the nodes).

MemoryDB allows us to get pricing that's based on the dataset size, and not the amount of operations, for a strongly consistent database. A database that is in-memory speed for reads and nearly in-memory speed for writes. I'm betting that's quite an interesting proposition for a number of developers.

Now if they would just add cross-region replication...


<!-- eraser-additional-content -->
## Diagrams
<!-- eraser-additional-files -->
<a href="/content/articles/2024-08-16_memorydb-sequence-diagram-1.eraserdiagram" data-element-id="QL_922PS3D-DmIRTYUES-"><img src="/.eraser/KoSBBMGFSpKInsUpJSNv___SdSlyWapPJMYH3JhtxQ9thJXxgb2___---diagram----e830e02d129aa37e38fb3fe12aae5fbf.png" alt="" data-element-id="QL_922PS3D-DmIRTYUES-" /></a>
<a href="/content/articles/2024-08-16_memorydb-sequence-diagram-2.eraserdiagram" data-element-id="mmicnQ_KqYdtGVoA3QGk6"><img src="/.eraser/KoSBBMGFSpKInsUpJSNv___SdSlyWapPJMYH3JhtxQ9thJXxgb2___---diagram----ded254ef1f37074afed9b9c93272e392.png" alt="" data-element-id="mmicnQ_KqYdtGVoA3QGk6" /></a>
<a href="/content/articles/2024-08-16_memorydb-sequence-diagram-3.eraserdiagram" data-element-id="d7nqZaePV52A6oMbOc0qK"><img src="/.eraser/KoSBBMGFSpKInsUpJSNv___SdSlyWapPJMYH3JhtxQ9thJXxgb2___---diagram----4974d00124b9840995e400c9b91b8d8a.png" alt="" data-element-id="d7nqZaePV52A6oMbOc0qK" /></a>
<!-- end-eraser-additional-files -->
<!-- end-eraser-additional-content -->
<!--- Eraser file: https://app.eraser.io/workspace/KoSBBMGFSpKInsUpJSNv --->